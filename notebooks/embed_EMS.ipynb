{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3558089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Classification\n",
    "\n",
    "# ## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6afcd24",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import esm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c704115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## augments\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "# Create the parser\n",
    "parser = argparse.ArgumentParser(description=\"Augments for embeding.\")\n",
    "\n",
    "# Add arguments\n",
    "parser.add_argument('--model', \n",
    "                    type=int, \n",
    "                    help=\"index of model to use from ESM 0:5: %(default)\", \n",
    "                    default=1)\n",
    "parser.add_argument('--cc', \n",
    "                    type=int, \n",
    "                    help=\"Chain choice. 0: sep, 1: together %(default)\", \n",
    "                    default=0)\n",
    "\n",
    "parser.add_argument('--file', \n",
    "                    type=str, \n",
    "                    help=\"file to embed %(default)\", \n",
    "                    default=\"anti\")\n",
    "parser.add_argument('--run_local', \n",
    "                    type=bool, \n",
    "                    help=\"to run local  %(default)\", \n",
    "                    default=True)\n",
    "\n",
    "# Parse the arguments\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Access the arguments\n",
    "# name = args.name\n",
    "model_args = args.model\n",
    "cc_args = args.cc\n",
    "file_args = args.file\n",
    "\n",
    "local_run = args.run_local\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ce162bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using local seting\n",
      "___file_args='cova'__\n",
      "__model_args=4___\n",
      "___cc_args=0___\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "if local_run :\n",
    "    print(\"using local seting\")\n",
    "\n",
    "    ##### model options\n",
    "    # ESM-2 8M model     # 0 \n",
    "    # ESM-2 35M model   # 1\n",
    "    # ESM-2 150M model  # 2\n",
    "    # ESM-2 650M model  # 3\n",
    "    # ESM-2 3B model    # 4 \n",
    "    # ESM-2 15B model   # 5\n",
    "    model_args = 4\n",
    "\n",
    "\n",
    "    ##### chain choce \n",
    "    # 0: sep, 1 together\n",
    "    # cc_args = 1     \n",
    "    cc_args = 0 \n",
    "\n",
    "    # file choice\n",
    "    # anti or cova as string\n",
    "    # file_args = \"anti\"\n",
    "    file_args = \"cova\" \n",
    "    print(f\"___{file_args=}__\\n__{model_args=}___\\n___{cc_args=}___\\n\")\n",
    "\n",
    "\n",
    "\n",
    "if model_args not in range(6):\n",
    "    print(\"model index not valid\")\n",
    "    if cc_args not in range(2):\n",
    "        print(\"cc choince not valid\")\n",
    "        if file_args not in [\"anti\",\"cova\"]:\n",
    "            print(\"lol\")\n",
    "            # sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")\n",
    "\n",
    "\n",
    "\n",
    "# ## \n",
    "\n",
    "# ## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d3cd12d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabel file in csv format\n"
     ]
    }
   ],
   "source": [
    "\n",
    "work_dir = os.getcwd()\n",
    "data_dir = os.path.join(work_dir, '../data')\n",
    "\n",
    "if file_args == \"anti\":\n",
    "    print(\"label file in excel format\")\n",
    "    data = pd.read_excel(os.path.join(data_dir, 'external/antibody_info.xlsx'), header=1)\n",
    "elif file_args == \"cova\":\n",
    "    print(\"unlabel file in csv format\")\n",
    "    data = pd.read_csv(os.path.join(data_dir, 'external/covabdab_search_results.csv'))\n",
    "else:\n",
    "    print(\"work in prgoess:\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2fe6e09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96     NaN\n",
       "98     NaN\n",
       "101    NaN\n",
       "Name: VL, dtype: object"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.iloc[[96, 98, 101]][\"VL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6dec7f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering for VL chains 'nan' \n",
      "nan in these index\n",
      "Index([96, 98, 101, 182, 1290], dtype='int64')\n",
      "there were drops this many rows: 5\n"
     ]
    }
   ],
   "source": [
    "### filter\n",
    "if file_args == \"cova\":\n",
    "    print(\"filtering for VL chains 'nan' \")\n",
    "    print(\"nan in these index\")\n",
    "    where_na = data[data['VL'].isna()].index\n",
    "    print(where_na)\n",
    "\n",
    "    print(f\"there were drops this many rows: {len(where_na)}\")\n",
    "    data = data.dropna(subset=['VL'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "615c374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geting chains in unlabel\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### # extacting file chains to embed\n",
    "\n",
    "if file_args == \"anti\":\n",
    "    print(\"geting chains in label\")\n",
    "    \n",
    "    # get the name and chains\n",
    "    chains = data[[\"Antibody  Name\",\"Heavy chain AA\",\"Light chain AA\"]]\n",
    "\n",
    "\n",
    "\n",
    "elif file_args == \"cova\":\n",
    "    print(\"geting chains in unlabel\")\n",
    "\n",
    "\n",
    "    # get the name and chains\n",
    "    chains = data[[\"Name\",\"VHorVHH\",\"VL\"]]\n",
    "    chains.columns = [\"Antibody  Name\",\"Heavy chain AA\",\"Light chain AA\"]\n",
    "\n",
    "else:\n",
    "    print(\"work in prgoess:\")\n",
    "\n",
    "\n",
    "# display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c076189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain mode: seperate \n",
      "\n",
      "________seting data long/seperate________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#chain mode,\n",
    "# Either seporate embeding or together by cls token as linker\n",
    "# Seporate heavy chains is frst.\n",
    "\n",
    "chain_choice = [\"seperate\",\"together\"]\n",
    "chain_mode = chain_choice[cc_args]\n",
    "print(f\"chain mode: {chain_mode} \\n\")\n",
    "\n",
    "df = chains\n",
    "\n",
    "# embed as septer\n",
    "if cc_args == 0: \n",
    "\n",
    "    print(\"________seting data long/seperate________\\n\\n\")\n",
    "\n",
    "\n",
    "    # pivot longer\n",
    "    df = pd.melt(df, id_vars=\"Antibody  Name\", value_vars=[\"Heavy chain AA\",\"Light chain AA\"],var_name = \"Chain ID\",value_name = \"chain AA\")\n",
    "    # new identy colun, combi of antibody and chain name\n",
    "    df[\"identifyer\"] = df[\"Antibody  Name\"].astype(str) + \" / \" + df[\"Chain ID\"]\n",
    "\n",
    "    # subset to mine colummn\n",
    "    df = df[[\"identifyer\",\"chain AA\"]]\n",
    "\n",
    "\n",
    "else: \n",
    "    # embed together with cls as divider\n",
    "    \n",
    "    # Token to insert in the middle\n",
    "    link_token = \"<cls>\"\n",
    "\n",
    "    # concat wtih toek\n",
    "    print(f\"______seting data together with: {link_token}______\\n\\n\")\n",
    "\n",
    "\n",
    "    # Combine the columns with the token in between\n",
    "    df['chain AA'] = df['Heavy chain AA'] + link_token + df['Light chain AA']\n",
    "\n",
    "    # lazy code\n",
    "    df[\"identifyer\"] = df[\"Antibody  Name\"]\n",
    "\n",
    "\n",
    "    # subset to relevant colmun\n",
    "    df = df[[\"identifyer\",\"chain AA\"]]\n",
    "\n",
    "\n",
    "# format to esm, list of tubles with (name, seq) \n",
    "esm_input = list(zip(*map(df.get, df[[\"identifyer\",\"chain AA\"]])))\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "# display(esm_input[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb830d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down/load model\n",
      "\n",
      "model to use: esm2_t36_3B_UR50D\n",
      "model found  in local file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/aipro/lib/python3.13/site-packages/esm/pretrained.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_data = torch.load(str(model_location), map_location=\"cpu\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(model_path):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# local file\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel found  in local file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     model, alphabet \u001b[38;5;241m=\u001b[39m \u001b[43mesm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_and_alphabet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# downloading\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownload file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/aipro/lib/python3.13/site-packages/esm/pretrained.py:26\u001b[0m, in \u001b[0;36mload_model_and_alphabet\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model_and_alphabet\u001b[39m(model_name):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# treat as filepath\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_and_alphabet_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m load_model_and_alphabet_hub(model_name)\n",
      "File \u001b[0;32m/anaconda/envs/aipro/lib/python3.13/site-packages/esm/pretrained.py:70\u001b[0m, in \u001b[0;36mload_model_and_alphabet_local\u001b[0;34m(model_location)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load from local path. The regression weights need to be co-located\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m model_location \u001b[38;5;241m=\u001b[39m Path(model_location)\n\u001b[0;32m---> 70\u001b[0m model_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m model_name \u001b[38;5;241m=\u001b[39m model_location\u001b[38;5;241m.\u001b[39mstem\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_regression_weights(model_name):\n",
      "File \u001b[0;32m/anaconda/envs/aipro/lib/python3.13/site-packages/torch/serialization.py:1368\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1367\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1376\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/anaconda/envs/aipro/lib/python3.13/site-packages/torch/serialization.py:1856\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1855\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1856\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1857\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m/anaconda/envs/aipro/lib/python3.13/site-packages/torch/serialization.py:1820\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1819\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1820\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/anaconda/envs/aipro/lib/python3.13/site-packages/torch/serialization.py:1780\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1777\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset : storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1779\u001b[0m     storage \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1780\u001b[0m         \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m         \u001b[38;5;241m.\u001b[39m_typed_storage()\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1783\u001b[0m     )\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"down/load model\\n\")\n",
    "torch.hub.set_dir(data_dir)\n",
    "\n",
    "esm2_model_names = [\n",
    "    'esm2_t6_8M_UR50D',    # ESM-2 8M model     # 0 \n",
    "    'esm2_t12_35M_UR50D',   # ESM-2 35M model   # 1\n",
    "    'esm2_t30_150M_UR50D',  # ESM-2 150M model  # 2\n",
    "    'esm2_t33_650M_UR50D',  # ESM-2 650M model  # 3\n",
    "    'esm2_t36_3B_UR50D',    # ESM-2 3B model    # 4 \n",
    "    'esm2_t48_15B_UR50D'    # ESM-2 15B model   # 5\n",
    "]\n",
    "\n",
    "model_name = esm2_model_names[model_args]\n",
    "print(f\"model to use: {model_name}\")\n",
    "\n",
    "model_path = data_dir+\"/checkpoints/\"+model_name+\".pt\"\n",
    "\n",
    "if True == os.path.isfile(model_path):\n",
    "    # local file\n",
    "    print(\"model found  in local file\")\n",
    "    model, alphabet = esm.pretrained.load_model_and_alphabet(model_path)\n",
    "else:\n",
    "    # downloading\n",
    "    print(\"download file\")\n",
    "    model, alphabet = esm.pretrained.load_model_and_alphabet(model_name)\n",
    "    \n",
    "\n",
    "# model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "# model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "# model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "# model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "# model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
    "# model, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeding\n",
      "0.0 % done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 10\n",
    "\n",
    "# data format needs to follow\n",
    "# data = [\n",
    "#     (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "#     (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "#     (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "#     (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "# ]\n",
    "\n",
    "\n",
    "# \n",
    "data_esm = esm_input#[0:10]  # for testing local\n",
    "\n",
    "\n",
    "\n",
    "# get label name, string length and boken of sequence\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data_esm)\n",
    "\n",
    "\n",
    "# used for sequence representaion.\n",
    "# batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "last_layer= sum(1 for layer in model.modules() if \"trans\" in str(type(layer)).lower())\n",
    "print(\"embeding\")\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     batch_tokens = batch_tokens.cuda()\n",
    "#     model = model.cuda()\n",
    "\n",
    "all_embeddings = []\n",
    "all_contacts = []\n",
    "\n",
    "\n",
    "number_batches = math.ceil(len(df)/batch_size)\n",
    "\n",
    "#local_test\n",
    "# number_batches = 2\n",
    "\n",
    "for batch_index in range(number_batches):\n",
    "    if batch_index % 10 ==0:\n",
    "        print(f\"{(batch_index/number_batches)*100} % done\")\n",
    "\n",
    "    batch_to_run = batch_tokens[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "\n",
    "    # Extract per-residue representations (on CPU)\n",
    "    # only the tokens are given the model, \n",
    "    # repr_layers only returns the 33 layers as representtion for every amino aids\n",
    "    # return_contacts predicts contracts between AA\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_to_run, repr_layers=[last_layer], return_contacts=True)\n",
    "    del results[\"logits\"]\n",
    "    del results[\"attentions\"]\n",
    "\n",
    "\n",
    "    # Extract the embeddings from the model output (layer 33 for ESM-1b)\n",
    "    embeddings = results['representations'][last_layer]  # Choose layer 33 for the embeddings\n",
    "\n",
    "    # Concatenate embeddings for this batch\n",
    "    all_embeddings.append(embeddings)\n",
    "    all_contacts.append(results['contacts'])\n",
    "\n",
    "# Concatenate embeddings from all batches into a single tensor\n",
    "concatenated_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "concatenated_contacts = torch.cat(all_contacts, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "results = {\"contacts\" : concatenated_contacts, \n",
    "           \"representations\" : {last_layer :concatenated_embeddings}}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Generate per-sequence representations via averaging\n",
    "# # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "# sequence_representations = []\n",
    "# for i, tokens_len in enumerate(batch_lens):\n",
    "#     sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "\n",
    "# attentions scored for pos to pos\n",
    "# # Look at the unsupervised self-attention map contact predictions\n",
    "# import matplotlib.pyplot as plt\n",
    "# for (_, seq), tokens_len, attention_contacts in zip(data, batch_lens, results[\"contacts\"]):\n",
    "#     plt.matshow(attention_contacts[: tokens_len, : tokens_len])\n",
    "#     plt.title(seq)\n",
    "#     plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name.split(\"_\")\n",
    "\n",
    "import pickle \n",
    "print(\"saving\")\n",
    "\n",
    "save_file = \"\"\n",
    "if file_args ==\"cova\":\n",
    "    save_file = \"cova_\"\n",
    "\n",
    "with open(f\"../data/interim/{save_file}embed_EMS_{model_name.split(\"_\")[2]}_{chain_mode}\", 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
