{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26845488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Classification\n",
    "\n",
    "# ## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa0c79c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import esm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3946f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## augments\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "# Create the parser\n",
    "parser = argparse.ArgumentParser(description=\"Augments for embeding.\")\n",
    "\n",
    "# Add arguments\n",
    "parser.add_argument('--model', \n",
    "                    type=int, \n",
    "                    help=\"index of model to use from ESM 0:5: %(default)\", \n",
    "                    default=1)\n",
    "parser.add_argument('--cc', \n",
    "                    type=int, \n",
    "                    help=\"Chain choice. 0: sep, 1: together %(default)\", \n",
    "                    default=0)\n",
    "\n",
    "parser.add_argument('--file', \n",
    "                    type=str, \n",
    "                    help=\"file to embed %(default)\", \n",
    "                    default=\"something\")\n",
    "parser.add_argument('--run_local', \n",
    "                    type=bool, \n",
    "                    help=\"to run local  %(default)\", \n",
    "                    default=False)\n",
    "\n",
    "# Parse the arguments\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Access the arguments\n",
    "# name = args.name\n",
    "model_args = args.model\n",
    "cc_args = args.cc\n",
    "file_args = args.file\n",
    "\n",
    "local_run = args.run_local\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d900ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using local seting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'local_run' not in globals():\n",
    "    print(\"using local seting\")\n",
    "    model_args = 4\n",
    "    cc_args = 0\n",
    "\n",
    "\n",
    "if model_args not in range(6):\n",
    "    print(\"model index not valid\")\n",
    "    if cc_args not in range(2):\n",
    "        print(\"cc choince not valid\")\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")\n",
    "\n",
    "\n",
    "\n",
    "# ## \n",
    "\n",
    "# ## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe94b8e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "work_dir = os.getcwd()\n",
    "data_dir = os.path.join(work_dir, '../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14714f3c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_excel(os.path.join(data_dir, 'external/antibody_info.xlsx'), header=1)\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16015d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain mode: seperate \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35449/2601118135.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['chain AA'] = df['Heavy chain AA'] + link_token + df['Light chain AA']\n",
      "/tmp/ipykernel_35449/2601118135.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"identifyer\"] = df[\"Antibody  Name\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#chain mode,\n",
    "# Either seporate embeding or together by cls token as linker\n",
    "# Seporate heavy chains is frst.\n",
    "\n",
    "chain_choice = [\"seperate\",\"together\"]\n",
    "chain_mode = chain_choice[cc_args]\n",
    "print(f\"chain mode: {chain_mode} \\n\")\n",
    "\n",
    "\n",
    "# get the name and chains\n",
    "df = data[[\"Antibody  Name\",\"Heavy chain AA\",\"Light chain AA\"]]\n",
    "\n",
    "# embed as septer\n",
    "if cc_args == 1: \n",
    "    # pivot longer\n",
    "    df = pd.melt(df, id_vars=\"Antibody  Name\", value_vars=[\"Heavy chain AA\",\"Light chain AA\"],var_name = \"Chain ID\",value_name = \"chain AA\")\n",
    "    # new identy colun, combi of antibody and chain name\n",
    "    df[\"identifyer\"] = df[\"Antibody  Name\"].astype(str) + \" / \" + df[\"Chain ID\"]\n",
    "\n",
    "    # subset to mine colummn\n",
    "    df = df[[\"identifyer\",\"chain AA\"]]\n",
    "\n",
    "\n",
    "else: \n",
    "    # embed together with cls as divider\n",
    "\n",
    "\n",
    "    # Token to insert in the middle\n",
    "    link_token = \"<cls>\"\n",
    "\n",
    "    # Combine the columns with the token in between\n",
    "    df['chain AA'] = df['Heavy chain AA'] + link_token + df['Light chain AA']\n",
    "\n",
    "    # lazy code\n",
    "    df[\"identifyer\"] = df[\"Antibody  Name\"]\n",
    "\n",
    "\n",
    "    # subset to relevant colmun\n",
    "    df = df[[\"identifyer\",\"chain AA\"]]\n",
    "\n",
    "\n",
    "# format to esm, list of tubles with (name, seq) \n",
    "df = list(zip(*map(df.get, df[[\"identifyer\",\"chain AA\"]])))\n",
    "\n",
    "# ## Embed \n",
    "\n",
    "# #### ESM 650M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727aecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down/load model\n",
      "\n",
      "model to use: esm2_t36_3B_UR50D\n",
      "model found  in local file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/aipro/lib/python3.13/site-packages/esm/pretrained.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_data = torch.load(str(model_location), map_location=\"cpu\")\n",
      "/anaconda/envs/aipro/lib/python3.13/site-packages/esm/pretrained.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  regression_data = torch.load(regression_location, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 2560, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-35): 36 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "      (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "      (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=1440, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "    (layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"down/load model\\n\")\n",
    "torch.hub.set_dir(data_dir)\n",
    "\n",
    "esm2_model_names = [\n",
    "    'esm2_t6_8M_UR50D',    # ESM-2 8M model\n",
    "    'esm2_t12_35M_UR50D',   # ESM-2 35M model\n",
    "    'esm2_t30_150M_UR50D',  # ESM-2 150M model\n",
    "    'esm2_t33_650M_UR50D',  # ESM-2 650M model\n",
    "    'esm2_t36_3B_UR50D',    # ESM-2 3B model\n",
    "    'esm2_t48_15B_UR50D'    # ESM-2 15B model\n",
    "]\n",
    "\n",
    "model_name = esm2_model_names[model_args]\n",
    "print(f\"model to use: {model_name}\")\n",
    "\n",
    "model_path = data_dir+\"/checkpoints/\"+model_name+\".pt\"\n",
    "\n",
    "if True == os.path.isfile(model_path):\n",
    "    # local file\n",
    "    print(\"model found  in local file\")\n",
    "    model, alphabet = esm.pretrained.load_model_and_alphabet(model_path)\n",
    "else:\n",
    "    # downloading\n",
    "    print(\"download file\")\n",
    "    model, alphabet = esm.pretrained.load_model_and_alphabet(model_name)\n",
    "    \n",
    "\n",
    "# model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "# model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "# model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "# model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "# model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
    "# model, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a4133",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeding\n",
      "0.0 % done\n",
      "3.2679738562091507 % done\n",
      "6.535947712418301 % done\n",
      "9.803921568627452 % done\n",
      "13.071895424836603 % done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 10\n",
    "\n",
    "# data format needs to follow\n",
    "# data = [\n",
    "#     (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "#     (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "#     (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "#     (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "# ]\n",
    "\n",
    "data = df#[0:10]  # for testing local\n",
    "\n",
    "\n",
    "\n",
    "# get label name, string length and boken of sequence\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "# used for sequence representaion.\n",
    "# batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "last_layer= sum(1 for layer in model.modules() if \"trans\" in str(type(layer)).lower())\n",
    "print(\"embeding\")\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     batch_tokens = batch_tokens.cuda()\n",
    "#     model = model.cuda()\n",
    "\n",
    "all_embeddings = []\n",
    "all_contacts = []\n",
    "\n",
    "\n",
    "number_batches = math.ceil(len(df)/batch_size)\n",
    "\n",
    "#local_test\n",
    "# number_batches = 2\n",
    "\n",
    "for batch_index in range(number_batches):\n",
    "    if batch_index % 10 ==0:\n",
    "        print(f\"{(batch_index/number_batches)*100} % done\")\n",
    "\n",
    "    batch_to_run = batch_tokens[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "\n",
    "    # Extract per-residue representations (on CPU)\n",
    "    # only the tokens are given the model, \n",
    "    # repr_layers only returns the 33 layers as representtion for every amino aids\n",
    "    # return_contacts predicts contracts between AA\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_to_run, repr_layers=[last_layer], return_contacts=True)\n",
    "    del results[\"logits\"]\n",
    "    del results[\"attentions\"]\n",
    "\n",
    "\n",
    "    # Extract the embeddings from the model output (layer 33 for ESM-1b)\n",
    "    embeddings = results['representations'][last_layer]  # Choose layer 33 for the embeddings\n",
    "\n",
    "    # Concatenate embeddings for this batch\n",
    "    all_embeddings.append(embeddings)\n",
    "    all_contacts.append(results['contacts'])\n",
    "\n",
    "# Concatenate embeddings from all batches into a single tensor\n",
    "concatenated_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "concatenated_contacts = torch.cat(all_contacts, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "results = {\"contacts\" : concatenated_contacts, \n",
    "           \"representations\" : {last_layer :concatenated_embeddings}}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Generate per-sequence representations via averaging\n",
    "# # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "# sequence_representations = []\n",
    "# for i, tokens_len in enumerate(batch_lens):\n",
    "#     sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "\n",
    "# attentions scored for pos to pos\n",
    "# # Look at the unsupervised self-attention map contact predictions\n",
    "# import matplotlib.pyplot as plt\n",
    "# for (_, seq), tokens_len, attention_contacts in zip(data, batch_lens, results[\"contacts\"]):\n",
    "#     plt.matshow(attention_contacts[: tokens_len, : tokens_len])\n",
    "#     plt.title(seq)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f6d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45e1ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name.split(\"_\")\n",
    "\n",
    "import pickle \n",
    "print(\"saving\")\n",
    "with open(f\"../data/interim/embed_EMS_{model_name.split(\"_\")[2]}_{chain_mode}\", 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "aipro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
