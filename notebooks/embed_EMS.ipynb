{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3558089e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# # Classification\n",
        "\n",
        "# ## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c6afcd24",
      "metadata": {
        "gather": {
          "logged": 1737509023997
        },
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m hard_dependencies, dependency, missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     np_version_under1p18 \u001b[38;5;28;01mas\u001b[39;00m _np_version_under1p18,\n\u001b[1;32m     24\u001b[0m     is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pandas/compat/__init__.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     is_numpy_dev,\n\u001b[1;32m     17\u001b[0m     np_array_datetime64_compat,\n\u001b[1;32m     18\u001b[0m     np_datetime64_compat,\n\u001b[1;32m     19\u001b[0m     np_version_under1p18,\n\u001b[1;32m     20\u001b[0m     np_version_under1p19,\n\u001b[1;32m     21\u001b[0m     np_version_under1p20,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     pa_version_under1p0,\n\u001b[1;32m     25\u001b[0m     pa_version_under2p0,\n\u001b[1;32m     26\u001b[0m     pa_version_under3p0,\n\u001b[1;32m     27\u001b[0m     pa_version_under4p0,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m PY38 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pandas/compat/numpy/__init__.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m     10\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pandas/util/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     Appender,\n\u001b[1;32m      3\u001b[0m     Substitution,\n\u001b[1;32m      4\u001b[0m     cache_readonly,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     hash_array,\n\u001b[1;32m      9\u001b[0m     hash_pandas_object,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name):\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pandas/util/_decorators.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache_readonly  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeprecate\u001b[39m(\n\u001b[1;32m     19\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     20\u001b[0m     alternative: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[[F], F]:\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pandas/_libs/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import esm\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9c704115",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ## augments\n",
        "\n",
        "\n",
        "import argparse\n",
        "\n",
        "# Create the parser\n",
        "parser = argparse.ArgumentParser(description=\"Augments for embeding.\")\n",
        "\n",
        "# Add arguments\n",
        "parser.add_argument('--model', \n",
        "                    type=int, \n",
        "                    help=\"index of model to use from ESM 0:5: %(default)\", \n",
        "                    default=1)\n",
        "parser.add_argument('--cc', \n",
        "                    type=int, \n",
        "                    help=\"Chain choice. 0: sep, 1: together %(default)\", \n",
        "                    default=0)\n",
        "\n",
        "parser.add_argument('--file', \n",
        "                    type=str, \n",
        "                    help=\"file to embed %(default)\", \n",
        "                    default=\"anti\")\n",
        "parser.add_argument('--run_local', \n",
        "                    type=bool, \n",
        "                    help=\"to run local  %(default)\", \n",
        "                    default=True)\n",
        "\n",
        "# Parse the arguments\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Access the arguments\n",
        "# name = args.name\n",
        "model_args = args.model\n",
        "cc_args = args.cc\n",
        "file_args = args.file\n",
        "\n",
        "local_run = args.run_local\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5ce162bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using local seting\n",
            "___file_args='cova'__\n",
            "__model_args=4___\n",
            "___cc_args=0___\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "if local_run :\n",
        "    print(\"using local seting\")\n",
        "\n",
        "    ##### model options\n",
        "    # ESM-2 8M model     # 0 \n",
        "    # ESM-2 35M model   # 1\n",
        "    # ESM-2 150M model  # 2\n",
        "    # ESM-2 650M model  # 3\n",
        "    # ESM-2 3B model    # 4 \n",
        "    # ESM-2 15B model   # 5\n",
        "    model_args = 4\n",
        "\n",
        "\n",
        "    ##### chain choce \n",
        "    # 0: sep, \n",
        "    # 1 together\n",
        "\n",
        "    cc_args = 0  \n",
        "    # cc_args = 1     \n",
        "\n",
        "    # file choice\n",
        "    # anti or cova as string\n",
        "    # file_args = \"anti\"\n",
        "    file_args = \"cova\" \n",
        "    print(f\"___{file_args=}__\\n__{model_args=}___\\n___{cc_args=}___\\n\")\n",
        "\n",
        "\n",
        "\n",
        "if model_args not in range(6):\n",
        "    print(\"model index not valid\")\n",
        "    if cc_args not in range(2):\n",
        "        print(\"cc choince not valid\")\n",
        "        if file_args not in [\"anti\",\"cova\"]:\n",
        "            print(\"lol\")\n",
        "            # sys.exit()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")\n",
        "\n",
        "\n",
        "\n",
        "# ## \n",
        "\n",
        "# ## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d3cd12d3",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unlabel file in csv format\n"
          ]
        }
      ],
      "source": [
        "\n",
        "work_dir = os.getcwd()\n",
        "data_dir = os.path.join(work_dir, '../data')\n",
        "\n",
        "if file_args == \"anti\":\n",
        "    print(\"label file in excel format\")\n",
        "    data = pd.read_excel(os.path.join(data_dir, 'external/antibody_info.xlsx'), header=1)\n",
        "elif file_args == \"cova\":\n",
        "    print(\"unlabel file in csv format\")\n",
        "    data = pd.read_csv(os.path.join(data_dir, 'external/covabdab_search_results.csv'))\n",
        "else:\n",
        "    print(\"work in prgoess:\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2fe6e09b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "96     NaN\n",
              "98     NaN\n",
              "101    NaN\n",
              "Name: VL, dtype: object"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "data.iloc[[96, 98, 101]][\"VL\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6dec7f4e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "filtering for VL chains 'nan' \n",
            "nan in these index\n",
            "Index([96, 98, 101, 182, 1290], dtype='int64')\n",
            "there were drops this many rows: 5\n"
          ]
        }
      ],
      "source": [
        "### filter\n",
        "if file_args == \"cova\":\n",
        "    print(\"filtering for VL chains 'nan' \")\n",
        "    print(\"nan in these index\")\n",
        "    where_na = data[data['VL'].isna()].index\n",
        "    print(where_na)\n",
        "\n",
        "    print(f\"there were drops this many rows: {len(where_na)}\")\n",
        "    data = data.dropna(subset=['VL'])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "615c374f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "geting chains in unlabel\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#### # extacting file chains to embed\n",
        "\n",
        "if file_args == \"anti\":\n",
        "    print(\"geting chains in label\")\n",
        "    \n",
        "    # get the name and chains\n",
        "    chains = data[[\"Antibody  Name\",\"Heavy chain AA\",\"Light chain AA\"]]\n",
        "\n",
        "\n",
        "\n",
        "elif file_args == \"cova\":\n",
        "    print(\"geting chains in unlabel\")\n",
        "\n",
        "\n",
        "    # get the name and chains\n",
        "    chains = data[[\"Name\",\"VHorVHH\",\"VL\"]]\n",
        "    chains.columns = [\"Antibody  Name\",\"Heavy chain AA\",\"Light chain AA\"]\n",
        "\n",
        "else:\n",
        "    print(\"work in prgoess:\")\n",
        "\n",
        "\n",
        "# display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c076189a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chain mode: seperate \n",
            "\n",
            "________seting data long/seperate________\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#chain mode,\n",
        "# Either seporate embeding or together by cls token as linker\n",
        "# Seporate heavy chains is frst.\n",
        "\n",
        "chain_choice = [\"seperate\",\"together\"]\n",
        "chain_mode = chain_choice[cc_args]\n",
        "print(f\"chain mode: {chain_mode} \\n\")\n",
        "\n",
        "df = chains\n",
        "\n",
        "# embed as septer\n",
        "if cc_args == 0: \n",
        "\n",
        "    print(\"________seting data long/seperate________\\n\\n\")\n",
        "\n",
        "\n",
        "    # pivot longer\n",
        "    df = pd.melt(df, id_vars=\"Antibody  Name\", value_vars=[\"Heavy chain AA\",\"Light chain AA\"],var_name = \"Chain ID\",value_name = \"chain AA\")\n",
        "    # new identy colun, combi of antibody and chain name\n",
        "    df[\"identifyer\"] = df[\"Antibody  Name\"].astype(str) + \" / \" + df[\"Chain ID\"]\n",
        "\n",
        "    # subset to mine colummn\n",
        "    df = df[[\"identifyer\",\"chain AA\"]]\n",
        "\n",
        "\n",
        "else: \n",
        "    # embed together with cls as divider\n",
        "    \n",
        "    # Token to insert in the middle\n",
        "    link_token = \"<cls>\"\n",
        "\n",
        "    # concat wtih toek\n",
        "    print(f\"______seting data together with: {link_token}______\\n\\n\")\n",
        "\n",
        "\n",
        "    # Combine the columns with the token in between\n",
        "    df['chain AA'] = df['Heavy chain AA'] + link_token + df['Light chain AA']\n",
        "\n",
        "    # lazy code\n",
        "    df[\"identifyer\"] = df[\"Antibody  Name\"]\n",
        "\n",
        "\n",
        "    # subset to relevant colmun\n",
        "    df = df[[\"identifyer\",\"chain AA\"]]\n",
        "\n",
        "\n",
        "# format to esm, list of tubles with (name, seq) \n",
        "esm_input = list(zip(*map(df.get, df[[\"identifyer\",\"chain AA\"]])))\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "# display(esm_input[:2])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9cb830d3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "down/load model\n",
            "\n",
            "model to use: esm2_t36_3B_UR50D\n",
            "model found  in local file\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/aipro/lib/python3.13/site-packages/esm/pretrained.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_data = torch.load(str(model_location), map_location=\"cpu\")\n",
            "/anaconda/envs/aipro/lib/python3.13/site-packages/esm/pretrained.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  regression_data = torch.load(regression_location, map_location=\"cpu\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ESM2(\n",
              "  (embed_tokens): Embedding(33, 2560, padding_idx=1)\n",
              "  (layers): ModuleList(\n",
              "    (0-35): 36 x TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "        (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "        (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "        (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
              "      (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
              "      (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (contact_head): ContactPredictionHead(\n",
              "    (regression): Linear(in_features=1440, out_features=1, bias=True)\n",
              "    (activation): Sigmoid()\n",
              "  )\n",
              "  (emb_layer_norm_after): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "    (layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "print(\"down/load model\\n\")\n",
        "torch.hub.set_dir(data_dir)\n",
        "\n",
        "esm2_model_names = [\n",
        "    'esm2_t6_8M_UR50D',    # ESM-2 8M model     # 0 \n",
        "    'esm2_t12_35M_UR50D',   # ESM-2 35M model   # 1\n",
        "    'esm2_t30_150M_UR50D',  # ESM-2 150M model  # 2\n",
        "    'esm2_t33_650M_UR50D',  # ESM-2 650M model  # 3\n",
        "    'esm2_t36_3B_UR50D',    # ESM-2 3B model    # 4 \n",
        "    'esm2_t48_15B_UR50D'    # ESM-2 15B model   # 5\n",
        "]\n",
        "\n",
        "model_name = esm2_model_names[model_args]\n",
        "print(f\"model to use: {model_name}\")\n",
        "\n",
        "model_path = data_dir+\"/checkpoints/\"+model_name+\".pt\"\n",
        "\n",
        "if True == os.path.isfile(model_path):\n",
        "    # local file\n",
        "    print(\"model found  in local file\")\n",
        "    model, alphabet = esm.pretrained.load_model_and_alphabet(model_path)\n",
        "else:\n",
        "    # downloading\n",
        "    print(\"download file\")\n",
        "    model, alphabet = esm.pretrained.load_model_and_alphabet(model_name)\n",
        "    \n",
        "\n",
        "# model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "# model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
        "# model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
        "# model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "# model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
        "# model, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()  # disables dropout for deterministic results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5712fafb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeding\n",
            "0.0 % done\n",
            "0.6357279084551812 % done\n",
            "1.2714558169103625 % done\n",
            "1.9071837253655435 % done\n",
            "2.542911633820725 % done\n",
            "3.178639542275906 % done\n",
            "3.814367450731087 % done\n",
            "4.4500953591862675 % done\n",
            "5.08582326764145 % done\n",
            "5.72155117609663 % done\n",
            "6.357279084551812 % done\n",
            "6.993006993006993 % done\n",
            "7.628734901462174 % done\n",
            "8.264462809917356 % done\n",
            "8.900190718372535 % done\n",
            "9.535918626827717 % done\n",
            "10.1716465352829 % done\n",
            "10.80737444373808 % done\n",
            "11.44310235219326 % done\n",
            "12.078830260648441 % done\n",
            "12.714558169103624 % done\n",
            "13.350286077558804 % done\n",
            "13.986013986013987 % done\n",
            "14.621741894469167 % done\n",
            "15.257469802924348 % done\n",
            "15.89319771137953 % done\n",
            "16.528925619834713 % done\n",
            "17.164653528289893 % done\n",
            "17.80038143674507 % done\n",
            "18.436109345200254 % done\n",
            "19.071837253655435 % done\n",
            "19.707565162110615 % done\n",
            "20.3432930705658 % done\n",
            "20.97902097902098 % done\n",
            "21.61474888747616 % done\n",
            "22.25047679593134 % done\n",
            "22.88620470438652 % done\n",
            "23.521932612841702 % done\n",
            "24.157660521296883 % done\n"
          ]
        }
      ],
      "source": [
        "\n",
        "batch_size = 10\n",
        "\n",
        "# data format needs to follow\n",
        "# data = [\n",
        "#     (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
        "#     (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
        "#     (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
        "#     (\"protein3\",  \"K A <mask> I S Q\"),\n",
        "# ]\n",
        "\n",
        "\n",
        "# \n",
        "data_esm = esm_input#[0:10]  # for testing local\n",
        "\n",
        "\n",
        "\n",
        "# get label name, string length and boken of sequence\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data_esm)\n",
        "\n",
        "\n",
        "# used for sequence representaion.\n",
        "# batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "last_layer= sum(1 for layer in model.modules() if \"trans\" in str(type(layer)).lower())\n",
        "print(\"embeding\")\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     batch_tokens = batch_tokens.cuda()\n",
        "#     model = model.cuda()\n",
        "\n",
        "all_embeddings = []\n",
        "all_contacts = []\n",
        "\n",
        "\n",
        "number_batches = math.ceil(len(df)/batch_size)\n",
        "\n",
        "#local_test\n",
        "# number_batches = 2\n",
        "\n",
        "for batch_index in range(number_batches):\n",
        "    if batch_index % 10 ==0:\n",
        "        print(f\"{(batch_index/number_batches)*100} % done\")\n",
        "\n",
        "    batch_to_run = batch_tokens[batch_index*batch_size:(batch_index+1)*batch_size]\n",
        "\n",
        "    # Extract per-residue representations (on CPU)\n",
        "    # only the tokens are given the model, \n",
        "    # repr_layers only returns the 33 layers as representtion for every amino aids\n",
        "    # return_contacts predicts contracts between AA\n",
        "\n",
        "    with torch.no_grad():\n",
        "        results = model(batch_to_run, repr_layers=[last_layer], return_contacts=True)\n",
        "    del results[\"logits\"]\n",
        "    del results[\"attentions\"]\n",
        "\n",
        "\n",
        "    # Extract the embeddings from the model output (layer 33 for ESM-1b)\n",
        "    embeddings = results['representations'][last_layer]  # Choose layer 33 for the embeddings\n",
        "\n",
        "    # Concatenate embeddings for this batch\n",
        "    all_embeddings.append(embeddings)\n",
        "    all_contacts.append(results['contacts'])\n",
        "\n",
        "# Concatenate embeddings from all batches into a single tensor\n",
        "concatenated_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "concatenated_contacts = torch.cat(all_contacts, dim=0)\n",
        "\n",
        "\n",
        "\n",
        "results = {\"contacts\" : concatenated_contacts, \n",
        "           \"representations\" : {last_layer :concatenated_embeddings}}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Generate per-sequence representations via averaging\n",
        "# # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "# sequence_representations = []\n",
        "# for i, tokens_len in enumerate(batch_lens):\n",
        "#     sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "\n",
        "\n",
        "# attentions scored for pos to pos\n",
        "# # Look at the unsupervised self-attention map contact predictions\n",
        "# import matplotlib.pyplot as plt\n",
        "# for (_, seq), tokens_len, attention_contacts in zip(data, batch_lens, results[\"contacts\"]):\n",
        "#     plt.matshow(attention_contacts[: tokens_len, : tokens_len])\n",
        "#     plt.title(seq)\n",
        "#     plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ee6b67",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model_name.split(\"_\")\n",
        "\n",
        "import pickle \n",
        "print(\"saving\")\n",
        "\n",
        "save_file = \"\"\n",
        "if file_args ==\"cova\":\n",
        "    save_file = \"cova_\"\n",
        "\n",
        "with open(f\"../data/interim/{save_file}embed_EMS_{model_name.split(\"_\")[2]}_{chain_mode}\", 'wb') as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "\n",
        "print(\"done\")"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# coding: utf-8",
      "executable": "/usr/bin/env python",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "aipro",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
